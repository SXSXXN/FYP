This project focuses on enhancing the robustness of distributed learning in multi-
agent systems. It introduces a novel strategy for computing trust between agents,
leveraging prediction comparisons as a basis to establish aggregate trust values at
each time step and accommodating time-varying trustworthiness. By selectively
cutting links based on trustworthiness, the network becomes more resilient to ma-
licious agents. Each agent then applies the Adapt-then-Combine (ATC) Diffusion
Strategy but solely aggregates the weights shared from neighbours it trusts. Exper-
imental evaluations and simulations are conducted on tasks of linear regression and
image classification using deep neural networks. The results demonstrate the effec-
tiveness of the trust-based approach in mitigating the influence of malicious agents
and it outperforms other robust algorithms even when there is a large proportion of
malicious agents.
